{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist DataEngineer (Inference Stage)\n",
    "\n",
    "****DataEngineerTeam****: Responsible for deploying the machine learning model by using batch workflows or online model serving, and managing the inference workflow or service\n",
    "- *Steps*\n",
    "    1. Import Scanflow and check the local environment\n",
    "    2. Develop scanflow application (workflows, agents)\n",
    "    3. Build scanflow application\n",
    "    4. Deploy scanflow environment\n",
    "    5. Download production models\n",
    "    6. Submit the metadata and artifacts to the central Scanflow-tracker\n",
    "    7. ****[Inference]****\n",
    "        1. Batch-inference (Argo)\n",
    "        2. Online-inference (Seldon)\n",
    "    **(step7 can be auto-managed by MAS)**\n",
    "    8. Clean environment\n",
    "- *Deliverables*\n",
    "    1. Built scanflow application metadata\n",
    "    2. DataEngineerTeam artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step1: Import Scanflow and check the local environment\n",
    "1. import scanflow\n",
    "    - For defining and building scanflow application, we need to import ScanflowClient\n",
    "    - For deploying scanflow application, we need to import ScanflowDeployerClient\n",
    "    - For saving artifacts, we need to import ScanflowTrackerClient\n",
    "2. check local environment\n",
    "    - For deploying scanflow application\n",
    "        - If user starts the notebook at local and has the privilege to submit object on Kubernetes. We don't need to configure \"SCANFLOW_SERVER_URI\"\n",
    "        - If user starts the notebook inside Kubernetes pod, or the local user does not have privilege to connect Kubernetes. We need to configure \"SCANFLOW_SERVER_URI\"\n",
    "    - For saving deliverables, we need to configure url of Scanflow-tracker on \"SCANFLOW_TRACKER_URI\" and url of Scanflow-local-tracker on \"SCANFLOW_TRACKER_LOCAL_URI\"\n",
    "    - If Scanflow-tracker is using S3 artifact storage, we need to configure S3 url \"MLFLOW_S3_ENDPOINT_URL\", username \"AWS_ACCESS_KEY_ID\" and password \"AWS_SECRET_ACCESS_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,'../..')\n",
    "\n",
    "import scanflow\n",
    "from scanflow.client import ScanflowClient\n",
    "from scanflow.client import ScanflowTrackerClient\n",
    "from scanflow.client import ScanflowDeployerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://172.30.0.50:46666\n",
      "http://172.30.0.50:46667\n",
      "http://172.30.0.50:43447\n",
      "admin\n",
      "admin123\n"
     ]
    }
   ],
   "source": [
    "from scanflow.tools import env\n",
    "print(env.get_env(\"SCANFLOW_SERVER_URI\"))\n",
    "print(env.get_env(\"SCANFLOW_TRACKER_URI\"))\n",
    "#print(env.get_env(\"SCANFLOW_TRACKER_LOCAL_URI\"))\n",
    "print(env.get_env(\"MLFLOW_S3_ENDPOINT_URL\"))\n",
    "print(env.get_env(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(env.get_env(\"AWS_SECRET_ACCESS_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Develop scanflow application\n",
    "\n",
    "  1. develop component (requirement.txt, script.py)\n",
    "  2. define scanflow workflows (Executor, Dependency, Workflow)\n",
    "  3. define agents to supervise the workflows\n",
    "  4. define scanflow application\n",
    "  ```bash\n",
    "     Application\n",
    "        - List: Workflow(DAG)\n",
    "                  - List: Executor\n",
    "                  - List: Dependency\n",
    "        - List: Agents(Web Services)\n",
    "  ```\n",
    "  \n",
    "  \n",
    "     For example:\n",
    "     \n",
    "  ```bash\n",
    "  mnist\n",
    "    - workflows\n",
    "       - load_data\n",
    "         - loaddata.py\n",
    "       - predictor-batch\n",
    "         - predictor.py\n",
    "       - detector-batch\n",
    "         - checker.py\n",
    "       - pick-data\n",
    "         - pick-data.py\n",
    "    - agents\n",
    "       - tracker\n",
    "       - checker\n",
    "       - improver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Develop scanflow workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App folder\n",
    "scanflow_path = \"/gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow\"\n",
    "app_dir = os.path.join(scanflow_path, \"examples/mnist/dataengineer\")\n",
    "app_name = \"mnist\"\n",
    "team_name = \"dataengineer\"\n",
    "\n",
    "# scanflow client\n",
    "client = ScanflowClient(\n",
    "              #if you defined \"SCANFLOW_SERVER_URI\", you dont need to provide this\n",
    "              #scanflow_server_uri=\"http://172.30.0.50:46666\",\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor\n",
    "executor1 = client.ScanflowExecutor(name='load-data', \n",
    "                      mainfile='loaddata.py',\n",
    "                      parameters={'app_name': app_name,\n",
    "                                  'team_name': 'data'})\n",
    "\n",
    "executor2 = client.ScanflowExecutor(name='predictor-batch', \n",
    "                      mainfile='predictor.py',\n",
    "                      parameters={'model_name': 'mnist_cnn',\n",
    "                                  'input_data': '/workflow/load-data/mnist/data/mnist_sample/test_images.npy', },\n",
    "                      base_image='modeling-cnn1')\n",
    "\n",
    "\n",
    "dependency1 = client.ScanflowDependency(dependee='load-data',\n",
    "                                    depender='predictor-batch')\n",
    "\n",
    "##workflow1 batch-inference\n",
    "## -- load_data\n",
    "##       -- predictor-batch\n",
    "workflow1 = client.ScanflowWorkflow(name='batch-inference', \n",
    "                     executors=[executor1, executor2],\n",
    "                     dependencies=[dependency1],\n",
    "                     output_dir = \"/workflow\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checker workflow\n",
    "executor3 = client.ScanflowExecutor(name='load-data', \n",
    "                      mainfile='loaddata.py',\n",
    "                      parameters={'experiment_name': 'checker',\n",
    "                                  'run_id': '',\n",
    "                                  'path': 'data'})\n",
    "\n",
    "executor4 = client.ScanflowExecutor(name='detector-batch', \n",
    "                      mainfile='checker.py',\n",
    "                      parameters={'model_name': 'mnist_detector',\n",
    "                                  'input_data': '/workflow/load-data/data/x_inference.npy'},\n",
    "                      base_image='checker')\n",
    "\n",
    "executor5 = client.ScanflowExecutor(name='pick-data', \n",
    "                      mainfile='pick-data.py',\n",
    "                      parameters={'e_inference': '/workflow/detector-batch/E_inference.csv',\n",
    "                                  'x_inference_artifact': '/workflow/load-data/data/x_inference.npy',\n",
    "                                  'y_inference_artifact': '/workflow/load-data/data/y_inference.npy'}, \n",
    "                      base_image='checker')\n",
    "\n",
    "dependency2 = client.ScanflowDependency(dependee='load-data',\n",
    "                                    depender='detector-batch')\n",
    "\n",
    "dependency3 = client.ScanflowDependency(dependee='detector-batch',\n",
    "                                    depender='pick-data')\n",
    "\n",
    "##workflow2\n",
    "## -- load-predicted-data\n",
    "##       -- detector-batch\n",
    "##          -- pick-data\n",
    "workflow2 = client.ScanflowWorkflow(name='detector-inference', \n",
    "                     executors=[executor3, executor4, executor5],\n",
    "                     dependencies=[dependency2, dependency3],\n",
    "                     output_dir = \"/workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Develop scanflow agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracker(monitor)\n",
    "trigger = client.ScanflowAgentSensor_IntervalTrigger(hours=1)\n",
    "sensor = client.ScanflowAgentSensor(name='count_number_of_predictions',\n",
    "                                    isCustom=True,\n",
    "                                    func_name='count_number_of_predictions',\n",
    "                                    trigger=trigger)\n",
    "tracker = client.ScanflowAgent(name='tracker',\n",
    "                              template='monitor',\n",
    "                              sensors=[sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checker(analyzer)\n",
    "# The frequency to retain the model is always in days. for testing we set 1 hour\n",
    "# trigger = client.ScanflowAgentSensor_IntervalTrigger(day=1)\n",
    "trigger = client.ScanflowAgentSensor_IntervalTrigger(hours=1)\n",
    "sensor = client.ScanflowAgentSensor(name='count_number_of_newdata',\n",
    "                                    isCustom=True,\n",
    "                                    func_name='count_number_of_newdata',\n",
    "                                    trigger=trigger)\n",
    "checker = client.ScanflowAgent(name='checker',\n",
    "                              template='analyzer',\n",
    "                              sensors=[sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#planner\n",
    "# The frequency to change the model is always in days. for testing we set 1 hour\n",
    "# trigger = client.ScanflowAgentSensor_IntervalTrigger(day=1)\n",
    "trigger = client.ScanflowAgentSensor_IntervalTrigger(hours=1)\n",
    "sensor = client.ScanflowAgentSensor(name='check_model_accuracy',\n",
    "                                    isCustom=True,\n",
    "                                    func_name='check_model_accuracy',\n",
    "                                    trigger=trigger)\n",
    "planner = client.ScanflowAgent(name='planner',\n",
    "                              template='planner',\n",
    "                              sensors=[sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executor\n",
    "executor = client.ScanflowAgent(name='executor',\n",
    "                              template='executor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Define scanflow application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = client.ScanflowApplication(app_name = app_name,\n",
    "                                 app_dir = app_dir,\n",
    "                                 team_name = team_name,\n",
    "                                 workflows=[workflow1, workflow2],\n",
    "                                 agents=[tracker, checker, planner, executor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27-May-21 10:53:32 -  INFO - Scanflowagent-tracker: {'name': 'tracker', 'template': 'monitor', 'sensors': [{'name': 'count_number_of_predictions', 'isCustom': True, 'func_name': 'count_number_of_predictions', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}\n",
      "27-May-21 10:53:32 -  INFO - Scanflowagent-checker: {'name': 'checker', 'template': 'analyzer', 'sensors': [{'name': 'count_number_of_newdata', 'isCustom': True, 'func_name': 'count_number_of_newdata', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}\n",
      "27-May-21 10:53:32 -  INFO - Scanflowagent-planner: {'name': 'planner', 'template': 'planner', 'sensors': [{'name': 'check_model_accuracy', 'isCustom': True, 'func_name': 'check_model_accuracy', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}\n",
      "27-May-21 10:53:32 -  INFO - Scanflowagent-executor: {'name': 'executor', 'template': 'executor', 'sensors': None, 'dockerfile': None, 'image': None}\n",
      "27-May-21 10:53:32 -  INFO - Scanflowapp: {'app_name': 'mnist', 'app_dir': '/gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer', 'team_name': 'dataengineer', 'workflows': [{'name': 'batch-inference', 'executors': [{'name': 'load-data', 'mainfile': 'loaddata.py', 'parameters': {'app_name': 'mnist', 'team_name': 'data'}, 'requirements': None, 'dockerfile': None, 'base_image': None, 'env': None, 'image': None}, {'name': 'predictor-batch', 'mainfile': 'predictor.py', 'parameters': {'model_name': 'mnist_cnn', 'input_data': '/workflow/load-data/mnist/data/mnist_sample/test_images.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'modeling-cnn1', 'env': None, 'image': None}], 'dependencies': [{'depender': 'predictor-batch', 'dependee': 'load-data', 'priority': 0}], 'output_dir': '/workflow'}, {'name': 'detector-inference', 'executors': [{'name': 'load-data', 'mainfile': 'loaddata.py', 'parameters': {'experiment_name': 'checker', 'run_id': '', 'path': 'data'}, 'requirements': None, 'dockerfile': None, 'base_image': None, 'env': None, 'image': None}, {'name': 'detector-batch', 'mainfile': 'checker.py', 'parameters': {'model_name': 'mnist_detector', 'input_data': '/workflow/load-data/data/x_inference.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'checker', 'env': None, 'image': None}, {'name': 'pick-data', 'mainfile': 'pick-data.py', 'parameters': {'e_inference': '/workflow/detector-batch/E_inference.csv', 'x_inference_artifact': '/workflow/load-data/data/x_inference.npy', 'y_inference_artifact': '/workflow/load-data/data/y_inference.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'checker', 'env': None, 'image': None}], 'dependencies': [{'depender': 'detector-batch', 'dependee': 'load-data', 'priority': 0}, {'depender': 'pick-data', 'dependee': 'detector-batch', 'priority': 0}], 'output_dir': '/workflow'}], 'agents': [{'name': 'tracker', 'template': 'monitor', 'sensors': [{'name': 'count_number_of_predictions', 'isCustom': True, 'func_name': 'count_number_of_predictions', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}, {'name': 'checker', 'template': 'analyzer', 'sensors': [{'name': 'count_number_of_newdata', 'isCustom': True, 'func_name': 'count_number_of_newdata', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}, {'name': 'planner', 'template': 'planner', 'sensors': [{'name': 'check_model_accuracy', 'isCustom': True, 'func_name': 'check_model_accuracy', 'trigger': {'weeks': 0, 'days': 0, 'hours': 1, 'minutes': 0, 'seconds': 0, 'start_date': None, 'end_date': None, 'timezone': None, 'jitter': None}, 'args': None, 'kwargs': None, 'next_run_time': None}], 'dockerfile': None, 'image': None}, {'name': 'executor', 'template': 'executor', 'sensors': None, 'dockerfile': None, 'image': None}], 'tracker': None}\n"
     ]
    }
   ],
   "source": [
    "dic = app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Step3: Build scanflow application (local)\n",
    "   \n",
    "  1. build images for Executor -> save to image registry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27-May-21 10:53:33 -  INFO - Building image 172.30.0.49:5000/tracker-agent\n",
      "27-May-21 10:53:33 -  INFO - [+] Image [172.30.0.49:5000/tracker-agent] not found in repository. Building a new one.\n",
      "27-May-21 10:53:33 -  INFO - tracker 's Dockerfile \n",
      "FROM 172.30.0.49:5000/scanflow-agent\n",
      "\n",
      "ENV AGENT_NAME tracker\n",
      "ENV AGENT_TYPE monitor\n",
      "\n",
      "COPY tracker /$AGENT_HOME/template/monitor\n",
      "\n",
      "27-May-21 10:53:33 -  INFO - [+] Dockerfile: [Dockerfile_scanflow_agent] was created successfully.\n",
      "27-May-21 10:53:33 -  INFO - dockerfile for using /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents/tracker/Dockerfile_scanflow_agent from /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [tracker] was built successfully. image_tag ['172.30.0.49:5000/tracker-agent:latest']\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [tracker] was pushed to registry successfully.\n",
      "27-May-21 10:53:34 -  INFO - Building image 172.30.0.49:5000/checker-agent\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [172.30.0.49:5000/checker-agent] not found in repository. Building a new one.\n",
      "27-May-21 10:53:34 -  INFO - checker 's Dockerfile \n",
      "FROM 172.30.0.49:5000/scanflow-agent\n",
      "\n",
      "ENV AGENT_NAME checker\n",
      "ENV AGENT_TYPE analyzer\n",
      "\n",
      "COPY checker /$AGENT_HOME/template/analyzer\n",
      "\n",
      "27-May-21 10:53:34 -  INFO - [+] Dockerfile: [Dockerfile_scanflow_agent] was created successfully.\n",
      "27-May-21 10:53:34 -  INFO - dockerfile for using /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents/checker/Dockerfile_scanflow_agent from /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [checker] was built successfully. image_tag ['172.30.0.49:5000/checker-agent:latest']\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [checker] was pushed to registry successfully.\n",
      "27-May-21 10:53:34 -  INFO - Building image 172.30.0.49:5000/planner-agent\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [172.30.0.49:5000/planner-agent] not found in repository. Building a new one.\n",
      "27-May-21 10:53:34 -  INFO - planner 's Dockerfile \n",
      "FROM 172.30.0.49:5000/scanflow-agent\n",
      "\n",
      "ENV AGENT_NAME planner\n",
      "ENV AGENT_TYPE planner\n",
      "\n",
      "COPY planner /$AGENT_HOME/template/planner\n",
      "\n",
      "27-May-21 10:53:34 -  INFO - [+] Dockerfile: [Dockerfile_scanflow_agent] was created successfully.\n",
      "27-May-21 10:53:34 -  INFO - dockerfile for using /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents/planner/Dockerfile_scanflow_agent from /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents\n",
      "27-May-21 10:53:34 -  INFO - [+] Image [planner] was built successfully. image_tag ['172.30.0.49:5000/planner-agent:latest']\n",
      "27-May-21 10:53:35 -  INFO - [+] Image [planner] was pushed to registry successfully.\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/executor-agent\n",
      "27-May-21 10:53:35 -  INFO - [+] Image [172.30.0.49:5000/executor-agent] not found in repository. Building a new one.\n",
      "27-May-21 10:53:35 -  INFO - executor 's Dockerfile \n",
      "FROM 172.30.0.49:5000/scanflow-agent\n",
      "\n",
      "ENV AGENT_NAME executor\n",
      "ENV AGENT_TYPE executor\n",
      "\n",
      "27-May-21 10:53:35 -  INFO - [+] Dockerfile: [Dockerfile_scanflow_agent] was created successfully.\n",
      "27-May-21 10:53:35 -  INFO - dockerfile for using /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents/executor/Dockerfile_scanflow_agent from /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer/agents\n",
      "27-May-21 10:53:35 -  INFO - [+] Image [executor] was built successfully. image_tag ['172.30.0.49:5000/executor-agent:latest']\n",
      "27-May-21 10:53:35 -  INFO - [+] Image [executor] was pushed to registry successfully.\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/load-data\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/predictor-batch\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/load-data\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/detector-batch\n",
      "27-May-21 10:53:35 -  INFO - Building image 172.30.0.49:5000/pick-data\n"
     ]
    }
   ],
   "source": [
    "build_app = client.build_ScanflowApplication(app = app, trackerPort=46669)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_app.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Deploy scanflow environment (local/incluster)\n",
    "  \n",
    "  1. Create k8s environment\n",
    "        - create namespace\n",
    "        - create RBAC, secret, configmap, PV, PVC\n",
    "        \n",
    "  2. Deploy scanflow-local-tracker (deployment, service)\n",
    " \n",
    "  3. Deploy scanflow-agents\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 20:02:09 -  INFO - loading kubernetes configuration from /gpfs/bsc_home/xpliu/.kube/config\n",
      "25-May-21 20:02:09 -  INFO - found local kubernetes configuration\n"
     ]
    }
   ],
   "source": [
    "deployerClient = ScanflowDeployerClient(user_type=\"local\",\n",
    "                                        deployer=\"seldon\",\n",
    "                                        k8s_config_file=\"/gpfs/bsc_home/xpliu/.kube/config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 20:02:10 -  INFO - [++]Creating env\n",
      "25-May-21 20:02:10 -  INFO - [++]Creating namespace \"scanflow-mnist-dataengineer\"\n",
      "25-May-21 20:02:10 -  INFO - create_namespace true\n",
      "25-May-21 20:02:10 -  INFO - [++]Creating Role for 'default service account'\n",
      "25-May-21 20:02:10 -  INFO - create_rolebinding info\n",
      "25-May-21 20:02:10 -  INFO - [++]Creating s3 secret {'AWS_ACCESS_KEY_ID': 'admin', 'AWS_SECRET_ACCESS_KEY': 'admin123', 'MLFLOW_S3_ENDPOINT_URL': 'http://minio.minio-system.svc.cluster.local:9000'}\n",
      "25-May-21 20:02:10 -  INFO - create_secret true\n",
      "25-May-21 20:02:10 -  INFO - [++]Creating tracker configmap {'TRACKER_STORAGE': 'postgresql://scanflow:scanflow123@postgresql-service.postgresql.svc.cluster.local/scanflow-mnist-dataengineer', 'TRACKER_ARTIFACT': 's3://scanflow/scanflow-mnist-dataengineer'}\n",
      "25-May-21 20:02:10 -  INFO - create_configmap true\n",
      "25-May-21 20:02:10 -  INFO - [++]Creating client configmap {'SCANFLOW_TRACKER_URI': 'http://scanflow-tracker-service.scanflow-system.svc.cluster.local', 'SCANFLOW_SERVER_URI': 'http://scanflow-server-service.scanflow-system.svc.cluster.local', 'SCANFLOW_TRACKER_LOCAL_URI': 'http://scanflow-tracker.scanflow-mnist-dataengineer.svc.cluster.local'}\n",
      "25-May-21 20:02:10 -  INFO - create_configmap true\n",
      "25-May-21 20:02:10 -  INFO - [+] Starting local tracker: [scanflow-tracker].\n",
      "25-May-21 20:02:10 -  INFO - create_deployment true\n",
      "25-May-21 20:02:10 -  INFO - [+] Created tracker Deployment True\n",
      "25-May-21 20:02:10 -  INFO - create_service true\n",
      "25-May-21 20:02:10 -  INFO - [+] Created tracker Service True\n",
      "25-May-21 20:02:10 -  INFO - [TEMPO: Because we dont have scanflow pip install now, we need to mount scanflow]\n",
      "25-May-21 20:02:10 -  INFO - create_pv true\n",
      "25-May-21 20:02:11 -  INFO - create_pvc true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await deployerClient.create_environment(app=build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: download prepared production mnist model\n",
    "   - download mnist-checker model (e.g., mnist_detector)\n",
    "   - download mnist model (e.g., mnist_cnn)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerClient = ScanflowTrackerClient(scanflow_tracker_local_uri=\"http://172.30.0.50:46669\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 16:50:30 -  INFO - Found credentials in environment variables.\n",
      "25-May-21 16:50:31 -  INFO - mnist--scanflow-model-datascience--{'training_dataset_len': 60000.0, 'accuracy': 0.9}--{}\n",
      "25-May-21 16:50:32 -  INFO - mnist_cnn exists\n",
      "2021/05/25 16:50:32 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: mnist_cnn, version 3\n"
     ]
    }
   ],
   "source": [
    "trackerClient.download_app_model(model_name=\"mnist_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 15:02:05 -  INFO - mnist--scanflow-model-datascience--{'history_val_loss': 0.08708706498146057, 'val_loss': 0.08708706498146057}--{'THRESHOLD_HIGH': '0.4934456065297127', 'THRESHOLD_LOW': '0.1665979415178299'}\n",
      "25-May-21 15:02:06 -  INFO - mnist_detector exists\n",
      "2021/05/25 15:02:06 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: mnist_detector, version 2\n"
     ]
    }
   ],
   "source": [
    "trackerClient.download_app_model(model_name=\"mnist_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Submit the metadata and artifacts to the central Scanflow-tracker\n",
    "\n",
    "  #### 6.1. Submit scanflowapp metadata\n",
    "  ```bash\n",
    "   mnist\n",
    "    - dataengineer\n",
    "     - workflows\n",
    "        - batch-inference.json\n",
    "        - detector-inference.json\n",
    "     - mnist.json\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackerClient = ScanflowTrackerClient(scanflow_tracker_local_uri=\"http://172.30.0.50:46669\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 16:01:31 -  INFO - Connecting tracking server uri: http://172.30.0.50:46667\n",
      "25-May-21 16:01:32 -  INFO - save app to artifact uri: s3://scanflow/1/f9609387d935405e8d0b0725d76d79fd/artifacts\n",
      "25-May-21 16:01:32 -  INFO - Scanflowapp: {'app_name': 'mnist', 'app_dir': '/gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer', 'team_name': 'dataengineer', 'workflows': [{'name': 'batch-inference', 'executors': [{'name': 'load-data', 'mainfile': 'loaddata.py', 'parameters': {'app_name': 'mnist', 'team_name': 'data'}, 'requirements': None, 'dockerfile': None, 'base_image': None, 'env': None, 'image': '172.30.0.49:5000/load-data:latest'}, {'name': 'predictor-batch', 'mainfile': 'predictor.py', 'parameters': {'model_name': 'mnist_cnn', 'input_data': '/workflow/load-data/mnist/data/mnist_sample/test_images.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'modeling-cnn1', 'env': None, 'image': '172.30.0.49:5000/predictor-batch:latest'}], 'dependencies': [{'depender': 'predictor-batch', 'dependee': 'load-data', 'priority': 0}], 'output_dir': '/workflow'}, {'name': 'detector-inference', 'executors': [{'name': 'load-data', 'mainfile': 'loaddata.py', 'parameters': {'experiment_name': 'checker', 'run_id': '', 'path': 'data'}, 'requirements': None, 'dockerfile': None, 'base_image': None, 'env': None, 'image': '172.30.0.49:5000/load-data:latest'}, {'name': 'detector-batch', 'mainfile': 'checker.py', 'parameters': {'model_name': 'mnist_detector', 'input_data': '/workflow/load-data/data/x_inference.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'checker', 'env': None, 'image': '172.30.0.49:5000/detector-batch:latest'}, {'name': 'pick-data', 'mainfile': 'pick-data.py', 'parameters': {'e_inference': '/workflow/detector-batch/E_inference.csv', 'x_inference_artifact': '/workflow/load-data/data/x_inference.npy', 'y_inference_artifact': '/workflow/load-data/data/y_inference.npy'}, 'requirements': None, 'dockerfile': None, 'base_image': 'checker', 'env': None, 'image': '172.30.0.49:5000/pick-data:latest'}], 'dependencies': [{'depender': 'detector-batch', 'dependee': 'load-data', 'priority': 0}, {'depender': 'pick-data', 'dependee': 'detector-batch', 'priority': 0}], 'output_dir': '/workflow'}], 'agents': None, 'tracker': {'image': '172.30.0.49:5000/scanflow-tracker', 'nodePort': 46669}}\n",
      "25-May-21 16:01:32 -  INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "trackerClient.save_app_meta(build_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Submit artifacts\n",
    "\n",
    "  ```bash\n",
    "mnist\n",
    "    - workflows\n",
    "       - load_data\n",
    "         - loaddata.py\n",
    "       - predictor-batch\n",
    "         - predictor.py\n",
    "       - detector-batch\n",
    "         - checker.py\n",
    "       - pick-data\n",
    "         - pick-data.py\n",
    "    - agents\n",
    "       - tracker\n",
    "       - checker\n",
    "       - improver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 13:38:16 -  INFO - Connecting tracking server uri: http://172.30.0.50:46667\n",
      "25-May-21 13:38:16 -  INFO - save app in /gpfs/bsc_home/xpliu/pv/jupyterhubpeini/scanflow/examples/mnist/dataengineer to artifact uri: s3://scanflow/1/4eeb989ac7164e0798f45c0fc4b0de9a/artifacts\n"
     ]
    }
   ],
   "source": [
    "trackerClient.save_app_artifacts(app_name=app_name, \n",
    "                                team_name=team_name, \n",
    "                                app_dir=app_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7: ****[Inference]****\n",
    "\n",
    "#### 7.1. Batch Inference\n",
    "\n",
    "Batch worklflow(workflow[0]:batch-inference) is defined by dataengineer, client could use it by changing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Online Inference\n",
    "Online workflow(workflow[1]:online-inference) is defined by dataengineer, first dataengineer should deploy the workflow as services by using seldon, then client could send request to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await deployerClient.run_workflow(app_name='mnist', \n",
    "                                  team_name='dataengineer',\n",
    "                                  workflow = build_app.workflows[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "### Step8: Clean scanflow environment\n",
    "  \n",
    "  1. delete environment\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-May-21 19:56:06 -  INFO - [++] Stopping tracker: [scanflow-tracker].\n",
      "25-May-21 19:56:06 -  INFO - delete_deployment true\n",
      "25-May-21 19:56:06 -  INFO - delete_service true\n",
      "25-May-21 19:56:06 -  INFO - [++]Delete tracker configmap scanflow-tracker-env\n",
      "25-May-21 19:56:06 -  INFO - delete_configmap true\n",
      "25-May-21 19:56:06 -  INFO - [++]Delete client configmap scanflow-client-env\n",
      "25-May-21 19:56:06 -  INFO - delete_configmap true\n",
      "25-May-21 19:56:06 -  INFO - [++]Delete s3 secret scanflow-secret\n",
      "25-May-21 19:56:06 -  INFO - delete_secret true\n",
      "25-May-21 19:56:06 -  INFO - [++]Delete rolebinding default-admin\n",
      "25-May-21 19:56:06 -  INFO - delete_rolebinding info\n",
      "25-May-21 19:56:06 -  INFO - [++]Delete namespace \"scanflow-mnist-dataengineer\"\n",
      "25-May-21 19:56:06 -  INFO - delete_namespace true\n",
      "25-May-21 19:56:06 -  INFO - delete_pvc true\n",
      "25-May-21 19:56:07 -  INFO - delete_pv true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await deployerClient.clean_environment(app=build_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
